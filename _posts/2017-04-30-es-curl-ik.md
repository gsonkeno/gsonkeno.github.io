---
layout: post
title:  "elasticsearch ik分词插件"
categories: Java
tags:  java elasticsearch ik
author: gaos
---

* content
{:toc}

对于国内软件开发行业现实情况来看，如果你的公司做分布式大数据平台服务时，使用了elasticsearch，那么存入es的文档一般有些字段会是中。用原生分词器`standard anayzer`就不太合适了，幸运的是`ik`分词器应运而生，它对于分词中文有极大的帮助。本文是针对elasticsearch 5.2版本的，其他版本类似。




## 1 安装
首先安装`elasticsearch`，安装`head`插件也是有很大帮助的，建议也安装，然后在github上下载`ik`分词器的[源码](https://github.com/medcl/elasticsearch-analysis-ik),教程在github上都有，不再赘述，简要说明一下。

下载后利用`maven`编译源代码，可能很慢，请耐心等待，编译后可获得编译后的`elasticsearch-analysis-ik-5.2.0.zip`文件，将其解压到elaticsearch下plugin/ik目录下，如果ik目录不存在，请创建，启动elasticsearch不报错即表示安装成功。

---

## 2 简单基本测试

- 建立索引
```
curl -XPUT "http://localhost:9200/index"
```

- 测试`ik_max_word`分词效果
```
curl -XPOST "http://localhost:9200/index/_analyze?analyzer=ik_max_word&text=中华人民共和国"
```
- 查看`ik_max_word`分词结果
```
{
    "tokens":[
        {
            "token":"中华人民共和国",
            "start_offset":0,
            "end_offset":7,
            "type":"CN_WORD",
            "position":0
        },
        {
            "token":"中华人民",
            "start_offset":0,
            "end_offset":4,
            "type":"CN_WORD",
            "position":1
        },
        {
            "token":"中华",
            "start_offset":0,
            "end_offset":2,
            "type":"CN_WORD",
            "position":2
        },
        {
            "token":"华人",
            "start_offset":1,
            "end_offset":3,
            "type":"CN_WORD",
            "position":3
        },
        {
            "token":"人民共和国",
            "start_offset":2,
            "end_offset":7,
            "type":"CN_WORD",
            "position":4
        },
        {
            "token":"人民",
            "start_offset":2,
            "end_offset":4,
            "type":"CN_WORD",
            "position":5
        },
        {
            "token":"共和国",
            "start_offset":4,
            "end_offset":7,
            "type":"CN_WORD",
            "position":6
        },
        {
            "token":"共和",
            "start_offset":4,
            "end_offset":6,
            "type":"CN_WORD",
            "position":7
        },
        {
            "token":"国",
            "start_offset":6,
            "end_offset":7,
            "type":"CN_CHAR",
            "position":8
        }
    ]
}
```

- 测试`ik_smart`分词效果
```
curl -XPOST "http://localhost:9200/index/_analyze?analyzer=ik_smart&text=中华人民共和国国歌"
```

- 查看`ik_smart`分词效果
```
{
    "tokens":[
        {
            "token":"中华人民共和国",
            "start_offset":0,
            "end_offset":7,
            "type":"CN_WORD",
            "position":0
        },
        {
            "token":"国歌",
            "start_offset":7,
            "end_offset":9,
            "type":"CN_WORD",
            "position":1
        }
    ]
}
```
总结:

- `ik_max_word`: 会将文本做最细粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,中华人民,中华,华人,人民共和国,人民,人,民,共和国,共和,和,国国,国歌”，会穷尽各种可能的组合；
- `ik_smart`: 会做最粗粒度的拆分，比如会将“中华人民共和国国歌”拆分为“中华人民共和国,国歌”

---

- 创建使用使用ik分析器的mapping
```
curl -XPOST http://localhost:9200/index/fulltext/_mapping -d'
{
    "fulltext": {
        "_all": {
            "analyzer": "ik_max_word",
            "search_analyzer": "ik_max_word",
            "term_vector": "no",
            "store": "false"
        },
        "properties": {
            "content": {
                "type": "text",
                "analyzer": "ik_max_word",
                "search_analyzer": "ik_max_word",
                "include_in_all": "true",
                "boost": 8
            }
        }
    }
}'
```

---

## 3 参考

- [github上的ik分词器](https://github.com/medcl/elasticsearch-analysis-ik)


